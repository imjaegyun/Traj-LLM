# modules/model.yaml

data_loader:
  num_agents: 6
  num_lanes: 6
  target_length: 12

sparse_encoder:
  input_dim: 4
  hidden_dim: 128
  output_dim: 128

high_level_model:
  llm_model_name: meta-llama/Llama-3.2-3B
  input_dim: 128
  output_dim: 3072
  use_lora: true
  lora_rank: 4

lane_aware_probability:
  input_dim: 3072  # 기존의 132에서 3072로 수정
  hidden_dim: 128
  num_lanes: 6

laplace_decoder:
  input_dim: 3108  # high_level_model.output_dim (3072) + lane_probabilities (36)
  output_dim: 2
  num_modes: 3

fusion:
  num_heads: 4
