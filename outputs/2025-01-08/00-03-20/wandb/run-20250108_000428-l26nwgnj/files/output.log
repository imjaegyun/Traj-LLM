LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                   | Type                         | Params | Mode
--------------------------------------------------------------------------------
0 | sparse_encoder         | SparseContextEncoder         | 658 K  | train
1 | mamba_layer            | MambaLayer                   | 330 K  | train
2 | high_level_model       | HighLevelInteractionModel    | 3.2 B  | train
3 | lane_probability_model | LaneAwareProbabilityLearning | 858 K  | train
4 | laplace_decoder        | MultimodalLaplaceDecoder     | 516    | train
--------------------------------------------------------------------------------
3.2 B     Trainable params
0         Non-trainable params
3.2 B     Total params
12,859.963Total estimated model params size (MB)
67        Modules in train mode
397       Modules in eval mode
Sanity Checking DataLoader 0:   0%|                                                                                                                                                                       | 0/2 [00:00<?, ?it/s]Input shape after unsqueeze: torch.Size([16, 15, 128])
Input shape after unsqueeze: torch.Size([16, 15, 128])
Input shape after unsqueeze: torch.Size([16, 15, 128])
Input shape after unsqueeze: torch.Size([16, 15, 128])
Error executing job with overrides: ['+task=train']
Traceback (most recent call last):
  File "/home/user/Traj-LLM/imjaegyun/Traj-LLM/train.py", line 26, in <module>
    main()
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/user/Traj-LLM/imjaegyun/Traj-LLM/train.py", line 15, in main
    train_main(config)  # train_main에 DictConfig 전달
  File "/home/user/Traj-LLM/imjaegyun/Traj-LLM/train/train_model.py", line 161, in train_main
    trainer.fit(model, train_loader, val_loader)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 539, in fit
    call._call_and_handle_interrupt(
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 575, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 982, in _run
    results = self._run_stage()
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1024, in _run_stage
    self._run_sanity_check()
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1053, in _run_sanity_check
    val_loop.run()
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 144, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 433, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 323, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/home/user/Traj-LLM/imjaegyun/Traj-LLM/train/train_model.py", line 113, in validation_step
    lane_probabilities, lane_predictions, mu, b = self(agent_inputs, lane_inputs)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/Traj-LLM/imjaegyun/Traj-LLM/train/train_model.py", line 69, in forward
    high_level_features = self.high_level_model(mamba_features)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/Traj-LLM/imjaegyun/Traj-LLM/models/high_level_interaction_model.py", line 23, in forward
    tokenized = self.tokenizer.batch_encode_plus(
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3141, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/user/anaconda3/envs/im_jg/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2762, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
