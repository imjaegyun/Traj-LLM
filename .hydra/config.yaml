train:
  lr: 0.0001
  epochs: 10
  weight_decay: 0.01
  batch_size: 64
  gpus: 1
  gradient_clip_val: 0.5
  lr_step: 10
  lr_gamma: 0.1
  k_values:
  - 5
  - 10
  lambda_lane: 1.0
test:
  batch_size: 1
  gpus: 1
  data_root: /home/user/data/kim_bh/Nuscenes
  checkpoint: /path/to/your/checkpoint.ckpt
  output_file: predictions.json
data:
  nuscenes_path: /home/user/data/kim_bh/Nuscenes
  train_split: train
  val_split: val
  test_split: test
wandb:
  project: Traj-LLM
  mode: online
tasks:
  train:
    lr: 0.001
    epochs: 10
    gpus: 1
    batch_size: 16
task: train
modules:
  data:
    nuscenes_path: /home/user/data/kim_bh/Nuscenes
    batch_size: 16
  data_loader:
    num_agents: 6
    num_lanes: 6
    target_length: 12
  sparse_encoder:
    input_dim: 4
    hidden_dim: 128
    output_dim: 128
  high_level_model:
    llm_model_name: meta-llama/Llama-3.2-3B
    input_dim: 128
    output_dim: 3072
    use_lora: true
    lora_rank: 4
  lane_aware_probability:
    input_dim: 3072
    hidden_dim: 128
    num_lanes: 6
  laplace_decoder:
    input_dim: 3108
    output_dim: 2
    num_modes: 3
  fusion:
    num_heads: 4
